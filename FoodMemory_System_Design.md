### **System Design: Food Instance Retrieval**

The system is designed to solve the food re-identification task. It is composed of three primary modules: the **Logging Module**, the **Memory Module**, and the **Retrieval Module**.

Assumption 1: omit video perception part
working system has Input: clips -> detected interacted food ->frame with food and segmentation of that food
now we assume we have that frame and segmentation of food

Assumption 2: assume knowing each clip is new food registering or not


### 1. Memory Module 


- **Core Technology:** A **Vector Database**.    
- **Data Schema:** Each entry in this database represents a single _observed instance_ of a food item and must contain:
    
    1. **`visual_embedding` (Vector):** The high-dimensional feature vector (e.g., a 512-dim vector) that visually represents the food item. This is the "feature" that will be used for similarity search.   
    2. **`caption` (String):** more verbose description generated by vlm
    3. **`semantic_label` (String):** The human-readable class name (e.g., "oil", "pizza", "apple"). This is taken from the VISOR noun class. 
    4. **`instance_id` (String/Int):** A unique identifier for this specific item. 
    5. **`source_reference` (JSON/String):** Metadata indicating where this item came from (e.g., `{video: "P06_03", frame: 4250}`). This allows the system to retrieve the original clip for verification.
        

Per assumption 2, we have separate logging and retrieving module. In practice, retrieval module is run 
### 2. Logging Module (logging an unseen food)

This module's job is to process an incoming "observation" (a video frame + annotation) and log it into the Memory Module. 


- **Input:** A VISOR annotation
        
- **Sub-Module: Visual Feature Extractor (VFE)**
    
    1. **Masking:** The VFE first applies the `pixel_mask` to the `video_frame`. This "blacks out" all background pixels, ensuring the resulting embedding is _only_ of the food item itself, not the surrounding scene.
        
    2. **Embedding:** The masked image is fed into a pre-trained visual model (e.g., a CLIP Image Encoder, DINO, etc.) to produce the `visual_embedding`.

- **Sub-Module: VLM caption (VFE)**
    
    1. Input is whole scene with transparent mask overlay, object label, calls api to generate caption
    
- **Sub-Module: Memory Interface**
    1. It issues a command to the Memory Module to **"INSERT"** this food and its associated data.
        

### 3. Retrieval Module 

This module's job is to take a query clip, process it, search the memory module, and return a ranked list of results.

- **Input:** A "query" data point (e.g., the "query" half of a WDTCF pair):
    
    - `query_frame` (Image)
        
    - `query_mask` (Image)
        
- **Sub-Module: Visual Feature Extractor (VFE)**
    
    - This module **must use the identical VFE component and model** as the Logging Module. This is critical to ensure that the query vector and the database vectors exist in the same embedding space.
        
    - It takes the `query_frame` and `query_mask`, performs masking and embedding, and produces a single `query_vector`.
        
- **Sub-Module: Vector Search Interface**
    
    1. This component takes the `query_vector` from the VFE.
        
    2. It issues a **"SEARCH"** command to the Memory Module.
        
    3. The Memory Module performs a k-NN search (e.g., by Cosine Similarity) and returns a ranked list of the Top-K most similar `instance_id`s from its database.'
    
        
- **Output:** A ranked list of `instance_id`s, which represents the system's best guess for "what is this item?" (e.g., `[wdtcf_item_007, distractor_451, distractor_992, ...]`).
- Another retrieval method is to use text embedding. here the text is either a generic label returned by an object classification model or the caption generated by VLM. 